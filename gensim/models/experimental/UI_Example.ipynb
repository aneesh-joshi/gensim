{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we will install the dependencies for running the Similarity Learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.models.experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73472b2be877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDRMM_TKS_Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.models.experimental'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from gensim.models.experimental import DRMM_TKS_Model\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to provide data in a format which is understood by the model.\n",
    "The model understands sentences as a list of words. \n",
    "Further, we need to give a :\n",
    " 1. Queries List\n",
    " 2. Candidate Document List\n",
    " 3. Correct Label List\n",
    "\n",
    "1 is a list of list of words\n",
    "2 and 3 is actually a list of list of list of words/ints\n",
    "\n",
    "Example:\n",
    "```\n",
    "queries = [\"When was Abraham Lincoln born ?\".split(), \n",
    "            \"When was the first World War ?\".split()]\n",
    "docs = [\n",
    "\t\t [\"Abraham Lincoln was the president of the United States of America\".split(),\n",
    "\t\t \"He was born in 1809\".split()],\n",
    "\t\t [\"The first world war was bad\".split(),\n",
    "\t\t \"It was fought in 1914\".split(),\n",
    "\t\t \"There were over a million deaths\".split()]\n",
    "       ]\n",
    "labels = [[0,\n",
    "           1],\n",
    "\t\t  [0,\n",
    "           1,\n",
    "           0]\n",
    "          ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset : WikiQA\n",
    "\n",
    "The WikiQA corpus is a set of question-answer pairs in which for every query there are several candidate documents of which none, one or more documents might be relevant.\n",
    "Relevance is purely binary, i.e., 1: relavant, 0: not relevant\n",
    "\n",
    "Sample data:\n",
    "```\n",
    "QuestionID\tQuestion\tDocumentID\tDocumentTitle\tSentenceID\tSentence\tLabel\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-0\tA partly submerged glacier cave on Perito Moreno Glacier .\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-1\tThe ice facade is approximately 60 m high\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-2\tIce formations in the Titlis glacier cave\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-3\tA glacier cave is a cave formed within the ice of a glacier .\t1\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-4\tGlacier caves are often called ice caves , but this term is properly used to describe bedrock caves that contain year-round ice.\t0\n",
    "```\n",
    "\n",
    "## Data Preprocessing\n",
    "We need to take the above text and make it into `queries, docs, labels` form\n",
    "We use the below code for that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill the below with wherever you have your WikiQACorpus Folder\n",
    "wikiqa_data_path = os.path.join('data', 'WikiQACorpus', 'WikiQA-train.tsv')\n",
    "\n",
    "\n",
    "def preprocess_sent(sent):\n",
    "    \"\"\"Utility function to lower, strip and tokenize each sentence\n",
    "    \n",
    "    Replace this function if you want to handle preprocessing differently\"\"\"\n",
    "    return re.sub(\"[^a-zA-Z0-9]\", \" \", sent.strip().lower()).split()\n",
    "\n",
    "# Defining some consants for .tsv reading\n",
    "QUESTION_ID_INDEX = 0\n",
    "QUESTION_INDEX = 1\n",
    "ANSWER_INDEX = 5\n",
    "LABEL_INDEX = 6\n",
    "\n",
    "with open(wikiqa_data_path, encoding='utf8') as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    data_rows = []\n",
    "    for row in tsv_reader:\n",
    "        data_rows.append(row)\n",
    "\n",
    "\n",
    "        \n",
    "document_group = []\n",
    "label_group = []\n",
    "\n",
    "n_relevant_docs = 0\n",
    "n_filtered_docs = 0\n",
    "\n",
    "queries = []\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "for i, line in enumerate(data_rows[1:], start=1):\n",
    "    if i < len(data_rows) - 1:  # check if out of bounds might occur\n",
    "        if data_rows[i][QUESTION_ID_INDEX] == data_rows[i + 1][QUESTION_ID_INDEX]:\n",
    "            document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "            label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "            n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "        else:\n",
    "            document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "            label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "\n",
    "            n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "\n",
    "            if n_relevant_docs > 0:\n",
    "                docs.append(document_group)\n",
    "                labels.append(label_group)\n",
    "                queries.append(preprocess_sent(data_rows[i][QUESTION_INDEX]))\n",
    "            else:\n",
    "                n_filtered_docs += 1\n",
    "\n",
    "            n_relevant_docs = 0\n",
    "            document_group = []\n",
    "            label_group = []\n",
    "\n",
    "    else:\n",
    "        # If we are on the last line\n",
    "        document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "        label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "        n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "\n",
    "        if n_relevant_docs > 0:\n",
    "            docs.append(document_group)\n",
    "            labels.append(label_group)\n",
    "            queries.append(preprocess_sent(data_rows[i][QUESTION_INDEX]))\n",
    "        else:\n",
    "            n_filtered_docs += 1\n",
    "            n_relevant_docs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a train-validation split\n",
    "At this point, it would be good to make a train-validation split so we can see how the model performs as it trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_queries, test_queries = queries[:int(len(queries)*0.8)], queries[int(len(queries)*0.8): ]\n",
    "train_docs, test_docs = docs[:int(len(docs)*0.8)], docs[int(len(docs)*0.8):]\n",
    "train_labels, test_labels = labels[:int(len(labels)*0.8)], labels[int(len(labels)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_queries), len(test_queries))\n",
    "print(len(train_docs), len(test_docs))\n",
    "print(len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "If we want to train the model with some pretrained word embeddingd like Glove, we will have to specify the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_path = os.path.join('data', 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to monitor the progress of training of the model.\n",
    "However, we can't rely on the metrics provided by keras as those metrics don't necessarily apply to Information Retrieval problems.\n",
    "\n",
    "We can additionally provide a validation dataset which will be tested after every epoch.\n",
    "\n",
    "Now that we have the preprocessed extracted data, training the model just takes one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "drmm_tks_model = DRMM_TKS_Model(train_queries, train_docs, train_labels, word_embedding_path=word_embedding_path,\n",
    "                                epochs=10, validation_data=[test_queries, test_docs, test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on new data\n",
    "\n",
    "The testing of the data can be done on completely unseen data using `model.predict(queries, docs)` where\n",
    "queries: list of list of words\n",
    "docs: list of list of list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "queries = [\"how are glacier caves formed ?\".split()]\n",
    "docs = [\"A partly submerged glacier cave on Perito Moreno Glacier\".split(),\n",
    "        \"A glacier cave is a cave formed within the ice of a glacier\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drmm_tks_model.predict(queries, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the correct answer has the higher similarity score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

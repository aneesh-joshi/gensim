{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset\n",
    "A script has been provided to download all the datasets required for running the below examples.\n",
    "It will dowload and unzip the WikiQA Corpus and the Quora Duplicate Questions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python data/get_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the word embeddings for the training.\n",
    "For this, we will use the Glove Embeddings.\n",
    "Luckily, [gensim-data](https://github.com/RaRe-Technologies/gensim-data) provides an easy interface for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "kv_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies for running the Similarity Learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from drmm_tks import DRMM_TKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to provide data in a format which is understood by the model.\n",
    "The model understands sentences as a list of words. \n",
    "Further, we need to give a :\n",
    " 1. Queries List\n",
    " 2. Candidate Document List\n",
    " 3. Correct Label List\n",
    "\n",
    "1 is a list of list of words\n",
    "2 and 3 is actually a list of list of list of words/ints\n",
    "\n",
    "Example:\n",
    "```\n",
    "queries = [\"When was Abraham Lincoln born ?\".split(), \n",
    "            \"When was the first World War ?\".split()]\n",
    "docs = [\n",
    "\t\t [\"Abraham Lincoln was the president of the United States of America\".split(),\n",
    "\t\t \"He was born in 1809\".split()],\n",
    "\t\t [\"The first world war was bad\".split(),\n",
    "\t\t \"It was fought in 1914\".split(),\n",
    "\t\t \"There were over a million deaths\".split()]\n",
    "       ]\n",
    "labels = [[0,\n",
    "           1],\n",
    "\t\t  [0,\n",
    "           1,\n",
    "           0]\n",
    "          ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset : WikiQA\n",
    "\n",
    "The WikiQA corpus is a set of question-answer pairs in which for every query there are several candidate documents of which none, one or more documents might be relevant.\n",
    "Relevance is purely binary, i.e., 1: relavant, 0: not relevant\n",
    "\n",
    "Sample data:\n",
    "\n",
    "QuestionID | Question | DocumentID | DocumentTitle | SentenceID | Sentence | Label\n",
    "-- | -- | -- | -- | -- | -- | --\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-0 | A partly submerged glacier cave on Perito Moreno Glacier . | 0\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-1 | The ice facade is approximately 60 m high | 0\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-2 | Ice formations in the Titlis glacier cave | 0\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-3 | A glacier cave is a cave formed within the ice of a glacier . | 1\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-4 | Glacier caves are often called ice caves , but this term is properly used to describe bedrock caves that contain year-round ice. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-0 | In physics , circular motion is a movement of an object along the circumference of a circle or rotation along a circular path. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-1 | It can be uniform, with constant angular rate of rotation (and constant speed), or non-uniform with a changing rate of rotation. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-2 | The rotation around a fixed axis of a three-dimensional body involves circular motion of its parts. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-3 | The equations of motion describe the movement of the center of mass of a body. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-4 | Examples of circular motion include: an artificial satellite orbiting the Earth at constant height, a stone which is tied to a rope and is being swung in circles, a car turning through a curve in a race track , an electron moving perpendicular to a uniform magnetic field , and a gear turning inside a mechanism. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-5 | Since the object's velocity vector is constantly changing direction, the moving object is undergoing acceleration by a centripetal force in the direction of the center of rotation. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-6 | Without this acceleration, the object would move in a straight line, according to Newton's laws of motion . | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to take the above text and make it into `queries, docs, labels` form. For this, we will create an iterable object with the below class which will allow the data to be streamed into the model as the need arises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWikiIterable:\n",
    "    \"\"\"\"Yields the next data point in the data set\n",
    "    A data point is: (query, doc_group, label_group)\n",
    "    where\n",
    "    query : list of str words\n",
    "    doc_group : list of docs\n",
    "                where a doc is a list of str words\n",
    "    label_group : list of int\n",
    "        The relevance between adjacent queries and docs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iter_type, fpath):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        iter_type : {'query', 'doc', 'label'}\n",
    "            the type of iterable to be yielded\n",
    "        fpath : str\n",
    "            path to the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # To map the `iter_type` to an index\n",
    "        self.type_translator = {'query': 0, 'doc': 1, 'label': 2}\n",
    "        self.iter_type = iter_type\n",
    "\n",
    "        with open(fpath, encoding='utf8') as tsv_file:\n",
    "            tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "            self.data_rows = []\n",
    "            self.data_rows = [row for row in tsv_reader]\n",
    "\n",
    "    def preprocess_sent(self, sent):\n",
    "        \"\"\"Utility function to lower, strip and tokenize each sentence\n",
    "\n",
    "        Replace this function if you want to handle preprocessing differently\"\"\"\n",
    "        return re.sub(\"[^a-zA-Z0-9]\", \" \", sent.strip().lower()).split()\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Defining some consants for .tsv reading\n",
    "        # They represent the columns of the respective values\n",
    "        QUESTION_ID_INDEX = 0\n",
    "        QUESTION_INDEX = 1\n",
    "        ANSWER_INDEX = 5\n",
    "        LABEL_INDEX = 6\n",
    "\n",
    "\n",
    "        # The group of documents and labels that belong to one question\n",
    "        document_group = []\n",
    "        label_group = []\n",
    "\n",
    "        # Number of relevant documents per query\n",
    "        n_relevant_docs = 0\n",
    "        # Number of filtered docs (query-doc pairs which have zero relevant docs)\n",
    "        n_filtered_docs = 0\n",
    "\n",
    "        # The data\n",
    "        queries = []\n",
    "        docs = []\n",
    "        labels = []\n",
    "\n",
    "        \n",
    "        # The code below goes through the data line by line\n",
    "        # It checks the current document id with the next document id\n",
    "        for i, line in enumerate(self.data_rows[1:], start=1):\n",
    "            if i < len(self.data_rows) - 1:  # check if out of bounds might occur\n",
    "                if self.data_rows[i][QUESTION_ID_INDEX] == self.data_rows[i + 1][QUESTION_ID_INDEX]:\n",
    "                    document_group.append(self.preprocess_sent(self.data_rows[i][ANSWER_INDEX]))\n",
    "                    label_group.append(int(self.data_rows[i][LABEL_INDEX]))\n",
    "                    n_relevant_docs += int(self.data_rows[i][LABEL_INDEX])\n",
    "                else:\n",
    "                    document_group.append(self.preprocess_sent(self.data_rows[i][ANSWER_INDEX]))\n",
    "                    label_group.append(int(self.data_rows[i][LABEL_INDEX]))\n",
    "\n",
    "                    n_relevant_docs += int(self.data_rows[i][LABEL_INDEX])\n",
    "\n",
    "                    if n_relevant_docs > 0:\n",
    "                        docs.append(document_group)\n",
    "                        labels.append(label_group)\n",
    "                        queries.append(self.preprocess_sent(self.data_rows[i][QUESTION_INDEX]))\n",
    "\n",
    "                        yield [queries[-1], document_group, label_group][self.type_translator[self.iter_type]]\n",
    "                    else:\n",
    "                        n_filtered_docs += 1\n",
    "\n",
    "                    n_relevant_docs = 0\n",
    "                    document_group = []\n",
    "                    label_group = []\n",
    "\n",
    "            else:\n",
    "                # If we are on the last line\n",
    "                document_group.append(self.preprocess_sent(self.data_rows[i][ANSWER_INDEX]))\n",
    "                label_group.append(int(self.data_rows[i][LABEL_INDEX]))\n",
    "                n_relevant_docs += int(self.data_rows[i][LABEL_INDEX])\n",
    "\n",
    "                if n_relevant_docs > 0:\n",
    "                    docs.append(document_group)\n",
    "                    labels.append(label_group)\n",
    "                    queries.append(self.preprocess_sent(self.data_rows[i][QUESTION_INDEX]))\n",
    "                    yield [queries[-1], document_group, label_group][self.type_translator[self.iter_type]]\n",
    "                else:\n",
    "                    n_filtered_docs += 1\n",
    "                    n_relevant_docs = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, will use the class to create objects of the training iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_iterable = MyWikiIterable('query', os.path.join( 'data', 'WikiQACorpus', 'WikiQA-train.tsv'))\n",
    "d_iterable = MyWikiIterable('doc', os.path.join('data', 'WikiQACorpus', 'WikiQA-train.tsv'))\n",
    "l_iterable = MyWikiIterable('label', os.path.join('data', 'WikiQACorpus', 'WikiQA-train.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we will also create a validation iterable so that we can see the progress the model makes as it trains.\n",
    "Notice that the path to the class is that of the test set. (Ideally, it should be the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_val_iterable = MyWikiIterable('query', os.path.join( 'data', 'WikiQACorpus', 'WikiQA-test.tsv'))\n",
    "d_val_iterable = MyWikiIterable('doc', os.path.join('data', 'WikiQACorpus', 'WikiQA-test.tsv'))\n",
    "l_val_iterable = MyWikiIterable('label', os.path.join('data', 'WikiQACorpus', 'WikiQA-test.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word embeddings\n",
    "The model needs some pre-trained word embeddings like Glove.\n",
    "\n",
    "We have 2 options for this:\n",
    "1. Use the [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html)\n",
    "2. Specify the path to the .txt\n",
    "\n",
    "This will serve as the `word_embedding` parameter in the model.\n",
    "\n",
    "Initially, we had loaded the KeyedVectors into `kv_model` using gensim-data and can now directly pass it.\n",
    "\n",
    "For now, however, let's just give the path. DRMM_TKS will internally create the KeyedVectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_path = os.path.join('data', 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "|\n",
    "We would like to monitor the progress of training of the model.\n",
    "However, we can't rely on the metrics provided by keras as those metrics don't necessarily apply to Information Retrieval problems.\n",
    "\n",
    "We can additionally provide a validation dataset which will be tested after every epoch.\n",
    "\n",
    "Now that we have the preprocessed extracted data, training the model just takes one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 19:55:12,258 : INFO : Starting Vocab Build\n",
      "2018-06-28 19:55:13,097 : INFO : Vocab Build Complete\n",
      "2018-06-28 19:55:13,098 : INFO : Vocab Size is 19894\n",
      "2018-06-28 19:55:13,098 : INFO : Building embedding index using pretrained word embeddings\n",
      "2018-06-28 19:55:17,398 : INFO : converting 400000 vectors from data/glove.6B.50d.txt to /tmp/tmp_word2vec.txt\n",
      "2018-06-28 19:55:18,190 : INFO : loading projection weights from /tmp/tmp_word2vec.txt\n",
      "2018-06-28 19:55:41,951 : INFO : loaded (400000, 50) matrix from /tmp/tmp_word2vec.txt\n",
      "2018-06-28 19:55:41,952 : INFO : The embeddings_index built from the given file has 400000 words of 50 dimensions\n",
      "2018-06-28 19:55:41,953 : INFO : Building the Embedding Matrix for the model's Embedding Layer\n",
      "2018-06-28 19:55:42,050 : INFO : There are 744 words out of 19894 (3.74%) not in the embeddings. Setting them to zero\n",
      "2018-06-28 19:55:42,051 : INFO : Adding additional words from the embedding file to embedding matrix\n",
      "2018-06-28 19:55:43,337 : INFO : Normalizing the word embeddings\n",
      "2018-06-28 19:55:44,919 : INFO : Embedding Matrix build complete. It now has shape (400746, 50)\n",
      "2018-06-28 19:55:44,920 : INFO : Pad word has been set to index 400744\n",
      "2018-06-28 19:55:44,921 : INFO : Unknown word has been set to index 400745\n",
      "2018-06-28 19:55:44,922 : INFO : Embedding index build complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vanessa/aneeshj/gensim/gensim_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 19:55:53,941 : WARNING : From /home/vanessa/aneeshj/gensim/gensim_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 50)      20037300    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 200, 200)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "top_k_layer_1 (TopKLayer)       (None, 200, 50)      0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200, 100)     5100        top_k_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200, 1)       101         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200, 1)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200, 1)       51          embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 200)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           reshape_2[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1)            0           dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 20,042,552\n",
      "Trainable params: 5,252\n",
      "Non-trainable params: 20,037,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 19:55:54,741 : INFO : Found 21 unknown words. Set them to unknown word index : 400745\n",
      "2018-06-28 19:55:54,812 : INFO : Found 263 unknown words. Set them to unknown word index : 400745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "128/128 [==============================] - 20s 154ms/step - loss: 0.2554 - acc: 0.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 19:56:19,135 : INFO : MAP: 0.47\n",
      "2018-06-28 19:56:19,150 : INFO : nDCG@1 : 0.32\n",
      "2018-06-28 19:56:19,172 : INFO : nDCG@3 : 0.47\n",
      "2018-06-28 19:56:19,185 : INFO : nDCG@5 : 0.53\n",
      "2018-06-28 19:56:19,202 : INFO : nDCG@10 : 0.59\n",
      "2018-06-28 19:56:19,224 : INFO : nDCG@20 : 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "128/128 [==============================] - 17s 134ms/step - loss: 0.2453 - acc: 0.5637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 19:56:40,081 : INFO : MAP: 0.48\n",
      "2018-06-28 19:56:40,095 : INFO : nDCG@1 : 0.33\n",
      "2018-06-28 19:56:40,110 : INFO : nDCG@3 : 0.48\n",
      "2018-06-28 19:56:40,124 : INFO : nDCG@5 : 0.54\n",
      "2018-06-28 19:56:40,138 : INFO : nDCG@10 : 0.60\n",
      "2018-06-28 19:56:40,152 : INFO : nDCG@20 : 0.62\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "drmm_tks_model = DRMM_TKS(q_iterable, d_iterable, l_iterable, word_embedding=word_embedding_path,\n",
    "                                epochs=2, validation_data=[q_val_iterable, d_val_iterable, l_val_iterable])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on new data\n",
    "\n",
    "The testing of the data can be done on completely unseen data using `model.predict(queries, docs)` where\n",
    "queries: list of list of words\n",
    "docs: list of list of list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "queries = [\"how are glacier caves formed ?\".split()]\n",
    "docs = [\"A partly submerged glacier cave on Perito Moreno Glacier\".split(),\n",
    "        \"A glacier cave is a cave formed within the ice of a glacier\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 19:56:41,449 : INFO : Found 0 unknown words. Set them to unknown word index : 400745\n",
      "2018-06-28 19:56:41,451 : INFO : Found 5 unknown words. Set them to unknown word index : 400745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50430447],\n",
       "       [0.57990086]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drmm_tks_model.predict(queries, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the correct answer has the higher similarity score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
